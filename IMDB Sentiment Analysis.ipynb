{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram and bag of words\n",
    "\n",
    "Set of all sequential words of n words or less. For example for the sentence \"hi im noah\", the bag of 2 gram would be:\n",
    "\n",
    "{hi,im,noah,hi im, im noah}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings {'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4, 'dog': 5, 'ate': 6, 'my': 7, 'homework': 8}\n",
      "One hot encoding: \n",
      " [[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding\n",
    "import numpy as np\n",
    "samples = ['the cat sat on the mat', 'the dog ate my homework']\n",
    "oneHotEncoding = {}\n",
    "for sample in samples:\n",
    "    split = sample.split()\n",
    "    for i in range(len(split)):\n",
    "        if split[i] not in oneHotEncoding:\n",
    "            oneHotEncoding[split[i]] = len(oneHotEncoding)\n",
    "print(\"Encodings\",oneHotEncoding)\n",
    "maxWords = 10\n",
    "results = np.zeros((len(samples),maxWords, len(oneHotEncoding)))\n",
    "\n",
    "for i,sample in enumerate(samples):\n",
    "    for x,word in enumerate(sample.split()):\n",
    "        results[i,x,oneHotEncoding[word]] = 1\n",
    "print(\"One hot encoding: \\n\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built in one hot encoding:\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "oneHot = tokenizer.sequences_to_matrix(sequences, mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings pack more information into less dimensions\n",
    "\n",
    "\n",
    "Learning word embeddings:\n",
    "\n",
    "Map words together geometrically that have similar meanins\n",
    "\n",
    "Perfect embedding space differs depending on problem looking at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embeddedLayer = Embedding(1000,64)\n",
    "#This layer is a dictionary to vector that represents word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "maxFeatures = 10000    #10000 most common words\n",
    "maxLen = 20            #Cut off each review after 20 first words\n",
    "\n",
    "(xTrain,yTrain), (xTest,yTest) = imdb.load_data(num_words = maxFeatures)\n",
    "print(xTrain[0])\n",
    "\n",
    "xTrain = preprocessing.sequence.pad_sequences(xTrain, maxlen = maxLen)\n",
    "xTest = preprocessing.sequence.pad_sequences(xTest, maxlen = maxLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  65,   16,   38, ...,   19,  178,   32],\n",
       "       [  23,    4, 1690, ...,   16,  145,   95],\n",
       "       [1352,   13,  191, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [  11, 1818, 7561, ...,    4, 3586,    2],\n",
       "       [  92,  401,  728, ...,   12,    9,   23],\n",
       "       [ 764,   40,    4, ...,  204,  131,    9]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.6582 - accuracy: 0.6409 - val_loss: 0.5997 - val_accuracy: 0.7044\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 934us/step - loss: 0.5235 - accuracy: 0.7583 - val_loss: 0.5204 - val_accuracy: 0.7304\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 874us/step - loss: 0.4530 - accuracy: 0.7914 - val_loss: 0.4987 - val_accuracy: 0.7476\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 882us/step - loss: 0.4175 - accuracy: 0.8102 - val_loss: 0.4957 - val_accuracy: 0.7578\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 833us/step - loss: 0.3923 - accuracy: 0.8227 - val_loss: 0.4949 - val_accuracy: 0.7590\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 827us/step - loss: 0.3720 - accuracy: 0.8351 - val_loss: 0.4993 - val_accuracy: 0.7596\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3542 - accuracy: 0.8464 - val_loss: 0.5064 - val_accuracy: 0.7594\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 878us/step - loss: 0.3376 - accuracy: 0.8557 - val_loss: 0.5128 - val_accuracy: 0.7586\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3221 - accuracy: 0.8630 - val_loss: 0.5195 - val_accuracy: 0.7520\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 949us/step - loss: 0.3063 - accuracy: 0.8713 - val_loss: 0.5265 - val_accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc5476b1e80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,8, input_length = maxLen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(xTrain,yTrain, epochs=10, validation_split = 0.2, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
